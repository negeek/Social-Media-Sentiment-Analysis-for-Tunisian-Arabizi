{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"word representation(fasttext).ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1SPF7ILXQubPU_phTGRpYy0rjgGciksqY","authorship_tag":"ABX9TyPlx1tnEf8NWB/fzpaldy+h"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"id":"MfloHWHXf7O-"},"source":["# import all libraries\r\n","import pandas as pd\r\n","import numpy as np\r\n","import seaborn as sns\r\n","import matplotlib.pyplot as plt\r\n","%matplotlib inline\r\n","import keras\r\n","import tensorflow as tf\r\n","from keras.preprocessing.text import Tokenizer\r\n","from keras.preprocessing.sequence import pad_sequences\r\n","from keras.models import Model, Sequential\r\n","from keras.layers import *\r\n","from keras.layers.embeddings import Embedding\r\n","from keras.optimizers import Adam\r\n","from keras.losses import sparse_categorical_crossentropy\r\n","from keras.callbacks import ModelCheckpoint\r\n","from sklearn.model_selection import train_test_split, StratifiedKFold\r\n","import requests, zipfile, io, os, math, codecs\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"id":"EW_VDUN5g4fv","executionInfo":{"elapsed":5086,"status":"ok","timestamp":1610891868805,"user":{"displayName":"Adebowale Daniel","photoUrl":"","userId":"11481151313452145685"},"user_tz":-60},"outputId":"b6adc3e5-28db-4c96-8b5b-6316a1685a2a"},"source":["'''zip_file_url = \"https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\"\r\n","r = requests.get(zip_file_url)\r\n","z = zipfile.ZipFile(io.BytesIO(r.content))\r\n","z.extractall()'''"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'zip_file_url = \"https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\"\\nr = requests.get(zip_file_url)\\nz = zipfile.ZipFile(io.BytesIO(r.content))\\nz.extractall()'"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":197},"id":"NqUZpTllgdfX","executionInfo":{"elapsed":6410,"status":"ok","timestamp":1610891870175,"user":{"displayName":"Adebowale Daniel","photoUrl":"","userId":"11481151313452145685"},"user_tz":-60},"outputId":"17502a1c-9cdb-4ad5-9021-04ee882c23ab"},"source":["train_path='/content/drive/My Drive/sentiment/Train.csv'\r\n","test_path='/content/drive/My Drive/sentiment/Test.csv'\r\n","sub_path='/content/drive/My Drive/sentiment/SampleSubmission.csv'\r\n","\r\n","train_df=pd.read_csv(train_path)\r\n","test_df=pd.read_csv(test_path)\r\n","sub_df=pd.read_csv(sub_path)\r\n","\r\n","train_df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ID</th>\n","      <th>text</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>13P0QT0</td>\n","      <td>3sbaaaaaaaaaaaaaaaaaaaa lek ou le seim riahi o...</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>SKCLXCJ</td>\n","      <td>cha3eb fey9elkoum menghir ta7ayoul ou kressi</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>V1TVXIJ</td>\n","      <td>bereau degage nathef ya slim walahi ya7chiw fi...</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>U0TTYY8</td>\n","      <td>ak slouma</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>68DX797</td>\n","      <td>entom titmanou lina a7na 3iid moubarik a7na ch...</td>\n","      <td>-1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        ID                                               text  label\n","0  13P0QT0  3sbaaaaaaaaaaaaaaaaaaaa lek ou le seim riahi o...     -1\n","1  SKCLXCJ       cha3eb fey9elkoum menghir ta7ayoul ou kressi     -1\n","2  V1TVXIJ  bereau degage nathef ya slim walahi ya7chiw fi...     -1\n","3  U0TTYY8                                          ak slouma      1\n","4  68DX797  entom titmanou lina a7na 3iid moubarik a7na ch...     -1"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"1hdR1kU0h9LH"},"source":["full_data= pd.concat([train_df,test_df], ignore_index=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZvltwOyBhhPr","outputId":"86ed4b5b-7dc3-4134-b488-04f056d81253"},"source":["from tqdm import tqdm\r\n","print('loading word embeddings...')\r\n","\r\n","embeddings_index = {}\r\n","f = codecs.open('/content/wiki-news-300d-1M.vec', encoding='utf-8')\r\n","\r\n","for line in tqdm(f):\r\n","    values = line.rstrip().rsplit(' ')\r\n","    word = values[0]\r\n","    coefs = np.asarray(values[1:], dtype='float32')\r\n","    embeddings_index[word] = coefs\r\n","f.close()\r\n","\r\n","print('found %s word vectors' % len(embeddings_index))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["1114it [00:00, 11135.34it/s]"],"name":"stderr"},{"output_type":"stream","text":["loading word embeddings...\n"],"name":"stdout"},{"output_type":"stream","text":["302643it [00:28, 10914.71it/s]"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"vuSZiVaZjpBk"},"source":["X_train,X_test,y_train,y_test = train_test_split(train_df['text'],train_df['label'],stratify=train_df['label'], test_size=0.2, random_state=14)\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"zDgF-BDFje5u","outputId":"e12a2fc1-9d75-4f7e-9cf6-86dc2a0b3dc8"},"source":["#Padding Function\r\n","s_len=25\r\n","def pad(x, length=None):\r\n","    if length is None:\r\n","        length = max([len(sentence) for sentence in x])\r\n","    return pad_sequences(x, maxlen = length, truncating='post',padding = 'post')\r\n","tokenizer = Tokenizer()\r\n","tokenizer.fit_on_texts(full_data['text'])\r\n","X_train = tokenizer.texts_to_sequences(X_train)\r\n","X_test = tokenizer.texts_to_sequences(X_test)\r\n","\r\n","word_index = tokenizer.word_index\r\n","\r\n","\r\n","X_train = pad(X_train, length=s_len)\r\n","X_test = pad(X_test,length=s_len)\r\n"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-600f959562f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncating\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpadding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'post'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'Tokenizer' is not defined"]}]},{"cell_type":"code","metadata":{"id":"uWkn_NN1hySf"},"source":["#embedding matrix\r\n","embed_dim=300\r\n","print('preparing embedding matrix...')\r\n","\r\n","words_not_found = []\r\n","embedding_matrix = np.zeros((len(word_index)+1, embed_dim))\r\n","\r\n","for word, i in word_index.items():\r\n","    embedding_vector = embeddings_index.get(word)\r\n","    if (embedding_vector is not None) and len(embedding_vector) > 0:\r\n","        # words not found in embedding index will be all-zeros.\r\n","        embedding_matrix[i] = embedding_vector\r\n","    else:\r\n","        words_not_found.append(word)\r\n","print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LmcMySrAoWNV"},"source":["y_train_enc=pd.get_dummies(y_train)\r\n","y_test_enc=pd.get_dummies(y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NG4b-H9rmc76"},"source":["\r\n","model = Sequential()\r\n","\r\n","model.add(Embedding(len(word_index)+1,embed_dim,input_length=s_len, weights=[embedding_matrix],trainable=True))\r\n","model.add(Bidirectional(LSTM(32,dropout=0.2, recurrent_dropout=0.2,return_sequences= True)))\r\n","model.add(Bidirectional(LSTM(32,dropout=0.3, recurrent_dropout=0.3,return_sequences= True)))\r\n","model.add(Bidirectional(LSTM(32,dropout=0.3, recurrent_dropout=0.3,return_sequences= True)))\r\n","model.add(Bidirectional(LSTM(64)))\r\n","model.add(Dense(3,activation='softmax'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"83U1hOqWoudu"},"source":["\r\n","model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.0005), metrics=['accuracy'])\r\n","model.fit(X_train, y_train_enc, validation_data=(X_test, y_test_enc), epochs=6, batch_size=5000)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CQEdgG0caVOg"},"source":["model.evaluate(X_test, y_test_enc)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gR6rGZs7oQJf"},"source":["test_seq = tokenizer.texts_to_sequences(test_df.text)\r\n","test_seq = pad(test_seq,length=s_len)\r\n","preds=model.predict(test_seq)\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TEwX2vmaQJYl"},"source":["preds=np.argmax(preds, axis=1)\r\n","corr_pred=[]\r\n","for i in preds:\r\n","  if i==0:\r\n","    corr_pred.append(-1)\r\n","  elif i==1:\r\n","    corr_pred.append(0)\r\n","  else:\r\n","    corr_pred.append(1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"78qjNc1yoiHz"},"source":["sub=pd.DataFrame({'ID':test_df.ID.values, 'label':corr_pred})\r\n","sub.to_csv('fasttext.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oE9DF0MoRJ6z"},"source":[" sub.label.value_counts()"],"execution_count":null,"outputs":[]}]}